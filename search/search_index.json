{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Probability Distributions","text":"<p>A comprehensive look at probability distributions components, applications in Python</p>"},{"location":"#introduction","title":"Introduction","text":"<ul> <li>An Introduction to Probability Distributions</li> </ul>"},{"location":"#distribution-functions","title":"Distribution Functions","text":"<ul> <li> <p>Probability Mass Function </p> </li> <li> <p>Probability Density Function </p> </li> </ul>"},{"location":"about/","title":"About","text":"<p>The documentattion is a collection of notes, tutorials and examples of Proability Distributions in support of Machine Learning and Data Science. My goal is note provide a comprehensive guide to the topic, but rather a quick reference to the most common distributions used in the field and some basic applications.</p>"},{"location":"introduction/","title":"Introductoin To Probability Distributions","text":"<p>A comprehensive look at probability distributions components, applications in Python</p>"},{"location":"introduction/#introduction","title":"Introduction","text":"<p>Probability distributions describe how the values of a random variable are distributed. They are fundamental in statistics and are categorized mainly into two types based on the nature of the random variable they describe:</p> <p>In short a distribution is a the possible values a variable can take and how frequently they occur Probability Distribution Notations:</p> <ul> <li>X,Y - random variables</li> <li>x,y, i - regular variables (specific realizations of the random variables)</li> <li>\\(E(X)\\) - Expected value of X</li> <li>\\(Var(X)\\) - Variance of X </li> </ul> <p>More notations will be introduced as we go along</p>"},{"location":"introduction/#types-of-probability-distributions","title":"Types of Probability Distributions","text":"<ol> <li> <p>Discrete Probability Distributions: These apply to discrete random variables, which are variables that take on a countable number of distinct values. The distribution of a discrete random variable is described by a Probability Mass Function (PMF), which gives the probability that the random variable takes on a specific value.</p> </li> <li> <p>Continuous Probability Distributions: These apply to continuous random variables, which can take on any value within a continuous range. The distribution of a continuous random variable is described by a Probability Density Function (PDF), which, when integrated over a range of values, gives the probability that the random variable falls within that range. The PDF itself does not give probabilities directly but rather the relative likelihood of the random variable being near a particular value.</p> </li> </ol> <p>Both types of distributions can be further characterized by their Cumulative Distribution Function (CDF), which gives the probability that the random variable is less than or equal to a certain value. For discrete random variables, the CDF is the cumulative sum of the PMF up to that value. For continuous random variables, the CDF is the integral of the PDF up to that value, representing the area under the PDF curve from negative infinity to that value.</p>"},{"location":"introduction/#random-variables","title":"Random Variables","text":"<p>A random variable is a variable whose value is subject to variations due to chance. It can take on a set of possible values, each with an associated probability. Random variables can be classified as either discrete or continuous, depending on the nature of the values they can take.</p> <p>A Random Variable in its simplest form is a function. In probability we often use random variables to represent random events. A Random Variable allows you to convert a real world experiment into a numerical value that we can perform some mathematics on.</p>"},{"location":"introduction/#random-variable-example","title":"Random Variable Example","text":"<p>Say you are sitting on a park bench, enjoying the nice weather as you people watch. You decide to count how many people walking by in a five minute period. You note the hair color and height of each person. One observation would be Person1 has brown hair and his height is 165 cm.  After you complete this experiment you ask yourself a few questions:</p> <ol> <li> <p>How many people walk passed ?  \u200310 people passed so therefore the sample space is 10. This sample space is also referred to as the outcome: \\(\\text{Outcome }\\Rightarrow \\Omega_1 = 10~people\\)  \u2003When working with the sample space in terms of distributons it is noted as a random variable: \\(\\text{Sample Space } \\Omega_1 \\Rightarrow \\text{Random Variable}X_1 = 10\\) </p> </li> <li> <p>What was the average height of the people who walked pass? \\(\\text{Outcome }\\Rightarrow \\Omega_2 = 165.32~cm\\) \\(\\text{Random Variable }\\Omega_2 \\Rightarrow X_2(x) = 165.32\\) </p> </li> </ol>"},{"location":"introduction/#probabilites-frequencies","title":"Probabilites (Frequencies)","text":"<p>Probability Frequency Distributions more commonly referred to as Probabilities is the measure of the likelihood of an outcome based on how often it is featured in the sample space (\\(\\Omega\\)). The probability of an event is a number between 0 and 1, where 0 indicates that the event will not occur and 1 indicates that the event will occur. The sum of the probabilities of all possible outcomes in the sample space is equal to 1.</p> <p>The Frequency Distribution Table below details the roll of 2 dice. The Sum column is the sum of the roll of the two dice from 2 ( 1 + 1) to 12 (6 + 6) with the highest frequency being the sum of 7. There are 6 different ways to roll a 7 resulting in a probability of \\(\\dfrac{1}{6}\\). Each probability is equal to the Frequency divided by the Sample Space \\(\\Omega\\). For example the frequency for 7 is 6, resulting in \\(\\dfrac{6}{36}\\) or \\(\\dfrac{1}{6}\\).  This is is the typical way to construct probabilities when we have a Finite amount of elements. Frequency Distribution Table for the Sum of Rolling Two Dice:</p> Sum Frequency Probability 2 1 \\(\\dfrac{1}{36}\\) 3 2 \\(\\dfrac{1}{18}\\) 4 3 \\(\\dfrac{1}{12}\\) 5 4 \\(\\dfrac{1}{9}\\) 6 5 \\(\\dfrac{5}{36}\\) 7 6 \\(\\dfrac{1}{6}\\) 8 5 \\(\\dfrac{5}{36}\\) 9 4 \\(\\dfrac{1}{9}\\) 10 3 \\(\\dfrac{1}{12}\\) 11 2 \\(\\dfrac{1}{18}\\) 12 1 \\(\\dfrac{1}{36}\\) <p></p>"},{"location":"introduction/#distribution-example","title":"Distribution Example","text":"<p>It helps me to start with an example when I am trying to grasp something that is not initially clear to me. So that is what I will do here with a Distribution. With the use of a fair coin, I will perform 8 trials which will entail 3 tosses. These 8 trials will be represented with the random variable X. The possible outcomes of the three tosses are: H = heads T = tails</p> <ul> <li>HHH</li> <li>HHT</li> <li>HTH</li> <li>THH</li> <li>TTH</li> <li>THT</li> <li>HTT</li> <li>TTT</li> </ul> <p>The Possible Outcomes Of X: There are FOUR possible values for X for each trial:</p> <ul> <li>you can flip THREE times and get ZERO Heads (TTT)</li> <li>you can flip THREE times and get ONE Heads (TTH, THT, HTT)</li> <li>you can flip THREE times and get TWO Heads (HTT, HTH, THH)</li> <li>you can flip THREE times and get THREE Heads (HHH)</li> </ul> <p>The Probabilities of the Possible Outcomes Of X \\(\\bigg(\\dfrac{\\text{successful trial(s)}}{\\text{total trials}}\\bigg)\\):</p> <ul> <li>Probability of getting 0 Heads (TTT): \\(P(X = 0) = \\dfrac{1}{8} = 0.125\\)</li> <li>Probability of getting 1 Heads (TTH, THT, HTT): \\(P(X = 3) = \\dfrac{3}{8} = 0.3755\\)</li> <li>Probability of getting 2 Heads (HTT, HTH, THH): \\(P(X = 3) = \\dfrac{3}{8} = 0.375\\)</li> <li>Probability of getting 3 Heads (HHH): \\(P(X = 3) = \\dfrac{1}{8} = 0.125\\) </li> </ul> <p>Plot The Distribution Of X</p> <p>The plot below shows the distribution of X. The x-axis represents the possible values of X (0, 1, 2, 3) and the y-axis represents the probability of each value occurring. The distribution is symmetric around the mean value of 1.5, with the highest probability of \\(0.375\\) occurring at X = 1 and X = 2 and a value of \\(0.125\\) at X = 0 and X =3. The distribution is discrete, as the random variable X can only take on integer values.</p> <p></p>"},{"location":"introduction/#conclusion","title":"Conclusion","text":"<p>Probability distributions are a fundamental concept in statistics that describe how the values of a random variable are distributed. There are two main types of probability distributions: discrete and continuous, which apply to discrete and continuous random variables, respectively. Discrete probability distributions are described by Probability Mass Functions (PMFs), while continuous probability distributions are described by Probability Density Functions (PDFs). Both types of distributions can be further characterized by their Cumulative Distribution Functions (CDFs), which give the probability that the random variable is less than or equal to a certain value. Probabilities are a measure of the likelihood of an outcome based on how often it is featured in the sample space, with the sum of the probabilities of all possible outcomes equal to 1.           </p> <p>End of Introduction</p>"},{"location":"pdf/","title":"Probability Density Function","text":"<p>The Probability DensiPy Function (PDF) is a function that describes the likelihood of a continuous random variable taking on a particular value. The PDF is used with continuous random variables, which can take on any value within a continuous range. Key points about the PDF include:</p> <ul> <li>The PDF itself does not give probabilities directly. Instead, it describes the relative likelihood of the random variable being near a particular value.</li> <li>The probability of the random variable falling within a particular range is given by the integral of the PDF over that range.</li> <li>The area under the entire PDF curve equals 1, representing the total probability of all possible outcomes.</li> </ul> <p>The PDF is crucial for understanding the distribution of continuous data and for calculating probabilities over intervals for continuous random variables.</p>"},{"location":"pdf/#probability-density-components","title":"Probability Density Components","text":"<ul> <li>observation - a data point in the sample</li> <li>probability - the likelihood of an outcome of an observation </li> <li>probability density - the relationship between the observation and its probability </li> <li>probability distribution - the overall shape of all the probability densities from the sample</li> <li>Probability Density Function (PDF) - performs calculations of probabilities for a specific outcomes of a random variable</li> </ul>"},{"location":"pdf/#pdf-estimation","title":"PDF Estimation","text":"<p>Given a random variable, we are interested in the density of its probabilities. For example, given a random sample of a variable, we might want to know things like the shape of the probability distribution, the most likely value, the spread of values, and other properties. Knowing the probability distribution for a random variable can help to calculate moments of the distribution, like the mean and variance, but can also be useful for other more general considerations, like determining whether an observation is unlikely or very unlikely and might be an outlier or anomaly. The problem is, we may not know the probability distribution for a random variable. We rarely do know the distribution because we don\u2019t have access to all possible outcomes for a random variable. In fact, all we have access to is a sample of observations. As such, we must select a probability distribution This problem is referred to as probability density estimation, or simply density estimation, as we are using the observations in a random sample to estimate the general density of probabilities beyond just the sample of data we have available.</p>"},{"location":"pdf/#density-visualization","title":"Density Visualization","text":"<p>Reviewing a histogram of a data sample with a range of different numbers of bins will help to identify whether the density looks like a common probability distribution / normal distribution or not  In most cases, you will see a unimodal distribution, such as the familiar bell shape of the normal, the flat shape of the uniform, or the descending or ascending shape of an exponential or Pareto distribution  You might also see complex distributions, such as multiple peaks, with different numbers of bins, referred to as a bimodal distribution, or multiple peaks, referred to as a multimodal distribution. You might also see a large spike in density for a given value or small range of values indicating outliers, often occurring on the tail of a distribution far away from the mean / center of the density </p>"},{"location":"pdf/#parametric-density-estimation","title":"Parametric Density Estimation","text":"<p>Identify Distribution  Get familiar with the common probability distributions as it will help you to identify a given distribution from a histogram  Once identified, you can attempt to estimate the density of the random variable with a chosen probability distribution. This can be achieved by estimating the parameters of the distribution from a random sample of data  For example, the normal distribution has two parameters:</p> <ul> <li>mean</li> <li>standard deviation</li> </ul> <p>Given these two parameters, we now know the probability distribution function. These parameters can be estimated from data by calculating the sample mean and sample standard deviation We refer to this process as parametric density estimation   Once we have estimated the density, we can check if it is a good fit. This can be done in many ways, such as:   * Plotting the density function and comparing the shape to the histogram  * Sampling the density function and comparing the generated sample to the real sample  * Using a statistical test to confirm the data fits the distribution   We can generate a random sample of 1000 observations from a normal distribution with a mean of 50 and standard deviation of 5:</p>"},{"location":"pmf/","title":"The PMF","text":"<p>The probability mass function (PMF) is a function that describes the probability of a discrete random variable taking on a particular value. For example, if we have a random variable X that represents the outcome of rolling a fair six-sided die, the PMF of X would assign a probability to each possible outcome (1, 2, 3, 4, 5, or 6).</p> <p>One roll of a dice where x denotes the number that the dice lands on, then the PDF for the outcome can be described as:</p> <p>One roll of a dice where x denotes the number that the dice lands on, then the PDF for the outcome can be described as: </p> <p>\\(P(x &lt; 1) : 0 \\\\ \\quad \\\\ P(x = 1) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x = 2) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x = 3) : \\dfrac{1}{6} \\\\ \\quad \\\\     P(x = 4) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x = 5) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x = 6) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x &gt; 6) : 0\\)</p> <p>Its important to note that x is a discrete variable in every instance. since x takes on only integer values.</p> <p> The probability of rolling a 1 on a six sided dice is 1/6 or .167 </p>"},{"location":"pmf/#the-pmf-and-the-cdf","title":"The PMF And The CDF","text":"<p>Cumulative Probability Function (CDF) measures the odds(probability) of two, three or more events happening A CDF tells us the probability that a random variable takes on a value less than or equal to x.   Example: One roll of a dice where x denotes the number that the dice lands on, then the CDF for the outcome can be described as: </p> <p>\\(P(x \\leq 0) : 0 \\\\ \\quad \\\\ P(x \\leq 1) : \\dfrac{1}{6} \\\\ \\quad \\\\ P(x \\leq 2) : \\dfrac{2}{6} \\\\ \\quad \\\\ P(x \\leq 3) : \\dfrac{3}{6} \\\\ \\quad \\\\ P(x \\leq 4) : \\dfrac{4}{6} \\\\ \\quad \\\\ P(x \\leq 5) : \\dfrac{5}{6} \\\\ \\quad \\\\  P(x \\leq 6) : \\dfrac{6}{6} \\\\ \\quad \\\\ P(x \\geq 6) : 0\\)</p> <p>Note that the probability that x is less than or equal to 6 is $\\dfrac{6}{6} = 1 $ This is because the dice will land on either 1, 2, 3, 4, 5, or 6 with 100%  probability</p> <p>This example uses a discrete random variable, however a CDF can also be used for a continuous random variable. </p> <p>CDF have the following properties:</p> <ul> <li>The probability of a random variable takes on a value less than the smallest possible value is zero.</li> <li>Example: A dice landing on a value less than 1 is zero.</li> </ul> <p>The probability that a random variable takes on a value less than or equal to the largest possible value is one.   This conveyed graphically below: </p> <p></p> <p>The probability of rolling a 1 on a six sided dice is \\(\\frac{1}{6}\\) or .167  Note that I also change the range to 0 - 1</p>"},{"location":"pmf/#pmf-properties","title":"PMF Properties","text":"<p>The PMF can only be used with discrete values. You cannot use continuous random values with PDFs directly, since the probability that x taking on any exact value iszero. For example, we want to know the probability that a burger from your favorite burger joint weigh:ws exactly a quarter of a pound (0.25 lbs) Since weight is a continuous variable, it can take on an infinite number of values. A burger may actually weigh 0.250001 lbs, or 0.24 lbs., or  0.24323 lbs. The probability that a burger weighs exactly 0.25 lbs is essentially zero.</p> <p>Below is a graphical representative of PMF and CDF side by side. Again for the PMF each roll of the dice has a 1/6 probability While the CDF expresses multiple probabilities which are \"cumulative\". </p> <p>The probability of rolling a 1 on a six sided dice is $\\frac{1}{6} $$ or .167$</p>"},{"location":"pmf/#pmf-and-cdf-exmaple","title":"PMF and CDF Exmaple","text":"<ul> <li>The probability of rolling a 4 is \\(\\frac{1}{6}\\) or \\(16.7\\%\\) (PDF)</li> <li>The probability of rolling a 1, 2, 3, or 4 is \\(\\frac{4}{6}\\) or \\(66.8 %\\) (CDF)</li> </ul> <p> Lets pretend that the dice is rigged in such a way that you cannot roll a 3 or 4.  This is what the discreet probability (PMF) and the cumulative probability (CDF) would look like </p> <p>On the left you the discreet probability with a rigged die that doesn't allow the roll of 3 or 4 The probability of rolling a 1, 2, 5, or 6 is \\(P(X 1, 2, 5, 6) = \\frac{1}{4}\\) or \\(25.0 \\%\\) </p> <p>On the right you have the cumulative probability of rolling a 1, 2, 5 or 6 which is \\(\\frac{4}{6}\\)  or \\(0.668 \\%\\)</p> <p>\\(P(X \\leq 4) = P(x = 1) + P(x = 2) + P(x = 3) + P(x = 4)\\)  Note that for the cumulative (CDF) the probability of getting 2 or less is the same for getting 3, 4 or less That is because there are no values for 3 or 4, so there is no cumulative effect&lt;br </p> <p></p> <p> On the left you the discreet probability with a rigged die that doesn't allow the roll of 3 or 4 \\(P(X 1, 2, 5, 6) = \\frac{1}{4}\\) or $ 25.0 \\%$  On the right you have the cumulative probability of rolling a 1, 2, 5 or 6 which is \\(\\frac{4}{6}\\) or \\(66.8 \\%\\) \\(P(X \\leq 4) = P(x = 1) + P(x = 2) + P(x = 3) + P(x = 4)\\)  Note that for the cumulative (CDF) the probability of getting 2 or less is the same for getting 3, 4 or less That is because there are no values for 3 or 4, so there is no cumulative effect :</p>"},{"location":"pmf/#pmf-applications","title":"PMF Applications","text":"<p>Probability Mass Functions (PMFs) are used in various fields and applications to describe the distribution of discrete random variables. Here is a list of a few of the many applications of PMFs:</p> <ul> <li> <p>Statistics and Data Analysis: PMFs are used to summarize and visualize the distribution of discrete data, helping in understanding the likelihood of different outcomes.</p> </li> <li> <p>Quality Control and Manufacturing: PMFs help in assessing the probability of defects in batches of products, enabling companies to maintain quality standards.</p> </li> <li> <p>Finance and Risk Management: PMFs are used to model the probability of discrete outcomes in financial scenarios, such as the number of defaults on loans or the occurrence of specific market conditions.</p> </li> <li> <p>Health Sciences: PMFs model the distribution of discrete outcomes in medical research and public health, such as the number of cases of a disease or the number of patients responding to a treatment.</p> </li> </ul>"},{"location":"bokeh/density1/","title":"Density1","text":"<p> <p> Array Values <ul> <li>Count: 2000</li> <li>Min: -9.513</li> <li>Max: 12.201</li> <li>mean: -0.021</li> <li>standard deviation: 2.985 </li> <li>75 percentile:  2.014 </li> <li>50 percentile:  0.043 </li> <li>25 percentile: -1.952 </li>"}]}